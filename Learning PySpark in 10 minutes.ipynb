{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PySpark in 10 minutes  \n",
    "\n",
    "The dataframe in PySpark is used extensively for cleaning of files dumped in AWS S3 buckets. The cleaned dataframe is then written back to the bucket in CSV or parquet format.\n",
    "\n",
    "I have used the famous Oracle `scott/tiger` datasets of ``EMP`` and ```DEPT```. I believe that working with 14 rows of known data gives an easier understanding of what is changing in the underlying dataset.\n",
    "\n",
    "The 2 files of `emp.csv` and 'dept.csv' are also present in here so easy access. I have changed the formats of the hiredate column to make cleaning noticeable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
