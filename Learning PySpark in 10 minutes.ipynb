{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark in 10 minutes  \n",
    "\n",
    "Dataframes (DF) in PySpark are used extensively for cleaning of data in files that are present in AWS S3 buckets. The cleaned dataframe is then written back to the bucket in CSV or parquet format.\n",
    "\n",
    "I have used the famous Oracle `scott/tiger` datasets of `EMP` and `DEPT`. I believe that working with 14 rows of known data gives an easier understanding of what is changing in the underlying dataset.\n",
    "\n",
    "The 2 files of [emp.csv](emp.csv) and [dept.csv](dept.csv) are also present in here so easy access. I have changed few formats of the hiredate column to make cleaning noticeable.\n",
    "\n",
    "1. Basic Exploration of the dataset\n",
    "   1. List the structure of the dataframe with data type of the columns\n",
    "   2. List the names of the columns in the dataframe\n",
    "   3. Count of rows in the DF\n",
    "   4. Count of columns in the DF\n",
    "   5. Explore a single column to understand the values it contains. Works better on data is numeric.\n",
    "2. Select distinct values of columns\n",
    "3. Filter out NULL values of a column.\n",
    "4. Add a column TotalSalary as addition of Salary and Commission.\n",
    "5. Do an INNER JOIN with dept file\n",
    "6. Create a SalaryGroup column that has 3 categories based on salary range.\n",
    "7. Apply in-build SQL function to columns\n",
    "8. Create UDF (User Defined Function) and apply to DF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required python libraries, create spark session and read emp.csv in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "|empno| ename|      job| mgr|   hiredate| sal| com|deptno|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902| 17-12-1980| 800|null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698| 02-20-1981|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839| 2-APR-1981|2975|null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28/SEP/1981|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-MAY-1981|2850|null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-JUN-1981|2450|null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566| 09/12/1982|3000|null|    20|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-SEP-1981|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|12-JAN-1983|1100|null|    20|\n",
      "| 7900| JAMES|    CLERK|7698| 3-DEC-1981| 950|null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566| 3-DEC-1981|3000|null|    20|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|\n",
      "+-----+------+---------+----+-----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "emp_df = spark.read.csv(\"emp.csv\",inferSchema=True,header=True)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the complete data of the emp table with format changes in the few of the hiredates.\n",
    "\n",
    "### 1. Basic Exploration of the dataset\n",
    "1. List the structure of the dataframe with data type of the columns\n",
    "2. List the names of the columns in the dataframe\n",
    "3. Count of rows in the DF\n",
    "4. Count of columns in the DF\n",
    "5. Explore a single column to understand the values it contains. Works better on data is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- com: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n",
      "['empno', 'ename', 'job', 'mgr', 'hiredate', 'sal', 'com', 'deptno']\n",
      "14\n",
      "8\n",
      "+-------+----------+\n",
      "|summary|  hiredate|\n",
      "+-------+----------+\n",
      "|  count|        14|\n",
      "|   mean|      null|\n",
      "| stddev|      null|\n",
      "|    min|02-20-1981|\n",
      "|    max|9-JUN-1981|\n",
      "+-------+----------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               sal|\n",
      "+-------+------------------+\n",
      "|  count|                14|\n",
      "|   mean| 2073.214285714286|\n",
      "| stddev|1182.5032235162716|\n",
      "|    min|               800|\n",
      "|    max|              5000|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|              com|\n",
      "+-------+-----------------+\n",
      "|  count|                4|\n",
      "|   mean|            550.0|\n",
      "| stddev|602.7713773341708|\n",
      "|    min|                0|\n",
      "|    max|             1400|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+-----+\n",
      "|summary|ename|\n",
      "+-------+-----+\n",
      "|  count|   14|\n",
      "|   mean| null|\n",
      "| stddev| null|\n",
      "|    min|ADAMS|\n",
      "|    max| WARD|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()                   # Notice the hiredate is also string\n",
    "print(emp_df.columns)                  # list headers in the DF\n",
    "print(emp_df.count())                  # 14 rows\n",
    "print(len(emp_df.columns))             # 8 columns\n",
    "emp_df.describe('hiredate').show()     # date is taken as string hence describe does not give correct MEAN, STDEV, MIN, MAX \n",
    "emp_df.describe('sal').show()          # works best for numeric column\n",
    "emp_df.describe('com').show()\n",
    "emp_df.describe('ename').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select distinct values of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+---------+\n",
      "|      job|\n",
      "+---------+\n",
      "|  ANALYST|\n",
      "| SALESMAN|\n",
      "|    CLERK|\n",
      "|  MANAGER|\n",
      "|PRESIDENT|\n",
      "+---------+\n",
      "\n",
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|  ANALYST|    20|\n",
      "|  MANAGER|    10|\n",
      "|  MANAGER|    30|\n",
      "|PRESIDENT|    10|\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    10|\n",
      "|  MANAGER|    20|\n",
      "|    CLERK|    30|\n",
      "+---------+------+\n",
      "\n",
      "+---------+------+\n",
      "|      job|deptno|\n",
      "+---------+------+\n",
      "|  ANALYST|    20|\n",
      "|  MANAGER|    10|\n",
      "|  MANAGER|    30|\n",
      "|PRESIDENT|    10|\n",
      "|    CLERK|    20|\n",
      "| SALESMAN|    30|\n",
      "|    CLERK|    10|\n",
      "|  MANAGER|    20|\n",
      "|    CLERK|    30|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(emp_df.select('job').distinct().count())  # Count of distinct values\n",
    "emp_df.select('job').distinct().show()          # Display those distinct values\n",
    "emp_df.select('job','deptno').distinct().show() # Works on multiple columns too\n",
    "emp_df.dropDuplicates(['job','deptno']).select('job','deptno').show()  # Alternate method using dropDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filter out NULL values of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+-----------+----+----+------+\n",
      "|empno| ename|     job| mgr|   hiredate| sal| com|deptno|\n",
      "+-----+------+--------+----+-----------+----+----+------+\n",
      "| 7499| ALLEN|SALESMAN|7698| 02-20-1981|1600| 300|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7654|MARTIN|SALESMAN|7698|28/SEP/1981|1250|1400|    30|\n",
      "| 7844|TURNER|SALESMAN|7698| 8-SEP-1981|1500|   0|    30|\n",
      "+-----+------+--------+----+-----------+----+----+------+\n",
      "\n",
      "+-----+------+--------+----+-----------+----+----+------+\n",
      "|empno| ename|     job| mgr|   hiredate| sal| com|deptno|\n",
      "+-----+------+--------+----+-----------+----+----+------+\n",
      "| 7499| ALLEN|SALESMAN|7698| 02-20-1981|1600| 300|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|22-FEB-1981|1250| 500|    30|\n",
      "| 7654|MARTIN|SALESMAN|7698|28/SEP/1981|1250|1400|    30|\n",
      "| 7844|TURNER|SALESMAN|7698| 8-SEP-1981|1500|   0|    30|\n",
      "+-----+------+--------+----+-----------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.dropna(subset=('com')).show()\n",
    "emp_df.filter(col('com').isNotNull()).show()     # Alternate method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add a column TotalSalary as addition of Salary and Commission.\n",
    "This is an example of appending a column to the existing dataframe. Notice that since commission is NULL for many rows, we use the `coalesce` function to `convert NULL to zero` so that addition is possible.\n",
    "* since `total_salary` does not exist, it gets created, had it existed then it would be overwritten\n",
    "* `col` is used to get column name\n",
    "* `coalesce` goes through its parameters till it finds a NOT NULL value\n",
    "* `lit` outputs a constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+------------+\n",
      "|empno| ename|      job| mgr|   hiredate| sal| com|deptno|total_salary|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+\n",
      "| 7369| SMITH|    CLERK|7902| 17-12-1980| 800|null|    20|       800.0|\n",
      "| 7499| ALLEN| SALESMAN|7698| 02-20-1981|1600| 300|    30|      1900.0|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|      1750.0|\n",
      "| 7566| JONES|  MANAGER|7839| 2-APR-1981|2975|null|    20|      2975.0|\n",
      "| 7654|MARTIN| SALESMAN|7698|28/SEP/1981|1250|1400|    30|      2650.0|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-MAY-1981|2850|null|    30|      2850.0|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-JUN-1981|2450|null|    10|      2450.0|\n",
      "| 7788| SCOTT|  ANALYST|7566| 09/12/1982|3000|null|    20|      3000.0|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|      5000.0|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-SEP-1981|1500|   0|    30|      1500.0|\n",
      "| 7876| ADAMS|    CLERK|7788|12-JAN-1983|1100|null|    20|      1100.0|\n",
      "| 7900| JAMES|    CLERK|7698| 3-DEC-1981| 950|null|    30|       950.0|\n",
      "| 7902|  FORD|  ANALYST|7566| 3-DEC-1981|3000|null|    20|      3000.0|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|      1300.0|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = emp_df.withColumn('total_salary',col('sal')+coalesce(emp_df['com'],lit('0')))\n",
    "emp_df.show()    # Original DF now has an additional column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Do an INNER JOIN with dept file\n",
    "`Outer join`, `union`, and `union all` can be done on similar lines.\n",
    "Read dept.csv into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df = spark.read.csv(\"dept.csv\",inferSchema=True,header=True)\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a new DF only for learning. This can be done in the same DF.\n",
    "* `sal > 10` is not required in the join, it is only there to show muti join condition\n",
    "* `drop` is done to remove the extra `deptno` column that would have the same values\n",
    "* `broadcast` is an optimization. The small table is broadcast to all nodes for faster join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+------------+----------+--------+\n",
      "|empno| ename|      job| mgr|   hiredate| sal| com|deptno|total_salary|     dname|     loc|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+----------+--------+\n",
      "| 7369| SMITH|    CLERK|7902| 17-12-1980| 800|null|    20|       800.0|  RESEARCH|  DALLAS|\n",
      "| 7499| ALLEN| SALESMAN|7698| 02-20-1981|1600| 300|    30|      1900.0|     SALES| CHICAGO|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|      1750.0|     SALES| CHICAGO|\n",
      "| 7566| JONES|  MANAGER|7839| 2-APR-1981|2975|null|    20|      2975.0|  RESEARCH|  DALLAS|\n",
      "| 7654|MARTIN| SALESMAN|7698|28/SEP/1981|1250|1400|    30|      2650.0|     SALES| CHICAGO|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-MAY-1981|2850|null|    30|      2850.0|     SALES| CHICAGO|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-JUN-1981|2450|null|    10|      2450.0|ACCOUNTING|NEW YORK|\n",
      "| 7788| SCOTT|  ANALYST|7566| 09/12/1982|3000|null|    20|      3000.0|  RESEARCH|  DALLAS|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|      5000.0|ACCOUNTING|NEW YORK|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-SEP-1981|1500|   0|    30|      1500.0|     SALES| CHICAGO|\n",
      "| 7876| ADAMS|    CLERK|7788|12-JAN-1983|1100|null|    20|      1100.0|  RESEARCH|  DALLAS|\n",
      "| 7900| JAMES|    CLERK|7698| 3-DEC-1981| 950|null|    30|       950.0|     SALES| CHICAGO|\n",
      "| 7902|  FORD|  ANALYST|7566| 3-DEC-1981|3000|null|    20|      3000.0|  RESEARCH|  DALLAS|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|      1300.0|ACCOUNTING|NEW YORK|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = emp_df.join(other=broadcast(dept_df),how='inner', on=[emp_df.deptno==dept_df.deptno, emp_df.sal>='10']).drop(dept_df.deptno)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create a SalaryGroup column that has 3 categories based on salary range.\n",
    "We add a new column `salaryGroup` based on multiple salary ranges. This is to show multiple `when` statements being used together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+------------+-----------+\n",
      "|empno| ename|      job| mgr|   hiredate| sal| com|deptno|total_salary|salaryGroup|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+-----------+\n",
      "| 7369| SMITH|    CLERK|7902| 17-12-1980| 800|null|    20|       800.0|  lowSalary|\n",
      "| 7499| ALLEN| SALESMAN|7698| 02-20-1981|1600| 300|    30|      1900.0|  avgSalary|\n",
      "| 7521|  WARD| SALESMAN|7698|22-FEB-1981|1250| 500|    30|      1750.0|  lowSalary|\n",
      "| 7566| JONES|  MANAGER|7839| 2-APR-1981|2975|null|    20|      2975.0|  avgSalary|\n",
      "| 7654|MARTIN| SALESMAN|7698|28/SEP/1981|1250|1400|    30|      2650.0|  lowSalary|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-MAY-1981|2850|null|    30|      2850.0|  avgSalary|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-JUN-1981|2450|null|    10|      2450.0|  avgSalary|\n",
      "| 7788| SCOTT|  ANALYST|7566| 09/12/1982|3000|null|    20|      3000.0| highSalary|\n",
      "| 7839|  KING|PRESIDENT|null|17-NOV-1981|5000|null|    10|      5000.0| highSalary|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-SEP-1981|1500|   0|    30|      1500.0|  lowSalary|\n",
      "| 7876| ADAMS|    CLERK|7788|12-JAN-1983|1100|null|    20|      1100.0|  lowSalary|\n",
      "| 7900| JAMES|    CLERK|7698| 3-DEC-1981| 950|null|    30|       950.0|  lowSalary|\n",
      "| 7902|  FORD|  ANALYST|7566| 3-DEC-1981|3000|null|    20|      3000.0| highSalary|\n",
      "| 7934|MILLER|    CLERK|7782|23-JAN-1982|1300|null|    10|      1300.0|  lowSalary|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = emp_df.withColumn('salaryGroup',when(emp_df[\"sal\"]<1550,'lowSalary')\n",
    "                  .when((emp_df[\"sal\"]>=1550) & (emp_df[\"sal\"]< 3000),'avgSalary')\n",
    "                  .when(emp_df[\"sal\"]>=3000,'highSalary').when(emp_df[\"sal\"].isNull(),'Error'))\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Apply in-build SQL function to columns\n",
    "1. Creating a new DF with select function\n",
    "2. Using SQL like functions to modify the Uppercase strings to Initcap\n",
    "3. Alias needs to be used to give appropriate name especially when a function is applied to the column\n",
    "4. Changing the column header for `sal` to `salary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+\n",
      "|empno| ename|      job|salary|\n",
      "+-----+------+---------+------+\n",
      "| 7369| Smith|    Clerk|   800|\n",
      "| 7499| Allen| Salesman|  1600|\n",
      "| 7521|  Ward| Salesman|  1250|\n",
      "| 7566| Jones|  Manager|  2975|\n",
      "| 7654|Martin| Salesman|  1250|\n",
      "| 7698| Blake|  Manager|  2850|\n",
      "| 7782| Clark|  Manager|  2450|\n",
      "| 7788| Scott|  Analyst|  3000|\n",
      "| 7839|  King|President|  5000|\n",
      "| 7844|Turner| Salesman|  1500|\n",
      "| 7876| Adams|    Clerk|  1100|\n",
      "| 7900| James|    Clerk|   950|\n",
      "| 7902|  Ford|  Analyst|  3000|\n",
      "| 7934|Miller|    Clerk|  1300|\n",
      "+-----+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smaller_df = emp_df.select(\"empno\",initcap(col(\"ename\")).alias(\"ename\"),initcap(col(\"job\")).alias(\"job\"),col(\"sal\").alias(\"salary\"))\n",
    "smaller_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create UDF (User Defined Function) and apply to DF\n",
    "User Defined Functions (UDF) should be the last resort. Builtin functions are tested and perform better.\n",
    "The above activity of creating the column `salaryGroup` could also have been done using an UDF.\n",
    "Using and UDf involves:\n",
    "1. Writing the function in Python\n",
    "2. Registering the function as UDF\n",
    "3. Invoking the UDF\n",
    "  \n",
    "As examples, 2 UDFs are created.\n",
    "1. Functionality similar to `initcap()` function, it will capitalize the first letter and lowercase the rest of the string.\n",
    "2. Apply a set of formats to hiredate to convert it into ISO type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalizeStr(mystr):\n",
    "    return mystr[0].upper() + mystr[1:].lower()\n",
    "\n",
    "capStrUDF = udf(lambda z: capitalizeStr(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import *\n",
    "\n",
    "def str2date (dt_str):\n",
    "    try:\n",
    "        return parse(dt_str)\n",
    "    except ValueError:\n",
    "        date_formats = ['%d-%m-%Y', '%d/%m/%Y', '%d.%m.%Y', '%m-%d-%Y', '%d-%b-%Y', '%d/%b/%Y','%Y-%m-%d'] \n",
    "        for date_format in date_formats:\n",
    "            try:\n",
    "                return datetime.strptime(dt_str,date_format) \n",
    "            except ValueError:\n",
    "                pass\n",
    "    raise ValueError('No valid date format found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertDateUDF = udf(lambda z: str2date(z), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+----+----+------+------------+-----------+-----------+\n",
      "|empno| ename|      job| mgr|   hiredate| sal| com|deptno|total_salary|salaryGroup|hiredateISO|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+-----------+-----------+\n",
      "| 7369| SMITH|    Clerk|7902| 17-12-1980| 800|null|    20|       800.0|  lowSalary| 1980-12-17|\n",
      "| 7499| ALLEN| Salesman|7698| 02-20-1981|1600| 300|    30|      1900.0|  avgSalary| 1981-02-20|\n",
      "| 7521|  WARD| Salesman|7698|22-FEB-1981|1250| 500|    30|      1750.0|  lowSalary| 1981-02-22|\n",
      "| 7566| JONES|  Manager|7839| 2-APR-1981|2975|null|    20|      2975.0|  avgSalary| 1981-04-02|\n",
      "| 7654|MARTIN| Salesman|7698|28/SEP/1981|1250|1400|    30|      2650.0|  lowSalary| 1981-09-28|\n",
      "| 7698| BLAKE|  Manager|7839| 1-MAY-1981|2850|null|    30|      2850.0|  avgSalary| 1981-05-01|\n",
      "| 7782| CLARK|  Manager|7839| 9-JUN-1981|2450|null|    10|      2450.0|  avgSalary| 1981-06-09|\n",
      "| 7788| SCOTT|  Analyst|7566| 09/12/1982|3000|null|    20|      3000.0| highSalary| 1982-09-12|\n",
      "| 7839|  KING|President|null|17-NOV-1981|5000|null|    10|      5000.0| highSalary| 1981-11-17|\n",
      "| 7844|TURNER| Salesman|7698| 8-SEP-1981|1500|   0|    30|      1500.0|  lowSalary| 1981-09-08|\n",
      "| 7876| ADAMS|    Clerk|7788|12-JAN-1983|1100|null|    20|      1100.0|  lowSalary| 1983-01-12|\n",
      "| 7900| JAMES|    Clerk|7698| 3-DEC-1981| 950|null|    30|       950.0|  lowSalary| 1981-12-03|\n",
      "| 7902|  FORD|  Analyst|7566| 3-DEC-1981|3000|null|    20|      3000.0| highSalary| 1981-12-03|\n",
      "| 7934|MILLER|    Clerk|7782|23-JAN-1982|1300|null|    10|      1300.0|  lowSalary| 1982-01-23|\n",
      "+-----+------+---------+----+-----------+----+----+------+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = emp_df.withColumn('hiredateISO',convertDateUDF(col('hiredate'))).withColumn('job',capStrUDF(col('job')))\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Rename column\n",
    "Renaming the column sal to salary and com to commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+-----------+------+----------+------+------------+-----------+-----------+\n",
      "|empno| ename|      job| mgr|   hiredate|salary|commission|deptno|total_salary|salaryGroup|hiredateISO|\n",
      "+-----+------+---------+----+-----------+------+----------+------+------------+-----------+-----------+\n",
      "| 7369| SMITH|    Clerk|7902| 17-12-1980|   800|      null|    20|       800.0|  lowSalary| 1980-12-17|\n",
      "| 7499| ALLEN| Salesman|7698| 02-20-1981|  1600|       300|    30|      1900.0|  avgSalary| 1981-02-20|\n",
      "| 7521|  WARD| Salesman|7698|22-FEB-1981|  1250|       500|    30|      1750.0|  lowSalary| 1981-02-22|\n",
      "| 7566| JONES|  Manager|7839| 2-APR-1981|  2975|      null|    20|      2975.0|  avgSalary| 1981-04-02|\n",
      "| 7654|MARTIN| Salesman|7698|28/SEP/1981|  1250|      1400|    30|      2650.0|  lowSalary| 1981-09-28|\n",
      "| 7698| BLAKE|  Manager|7839| 1-MAY-1981|  2850|      null|    30|      2850.0|  avgSalary| 1981-05-01|\n",
      "| 7782| CLARK|  Manager|7839| 9-JUN-1981|  2450|      null|    10|      2450.0|  avgSalary| 1981-06-09|\n",
      "| 7788| SCOTT|  Analyst|7566| 09/12/1982|  3000|      null|    20|      3000.0| highSalary| 1982-09-12|\n",
      "| 7839|  KING|President|null|17-NOV-1981|  5000|      null|    10|      5000.0| highSalary| 1981-11-17|\n",
      "| 7844|TURNER| Salesman|7698| 8-SEP-1981|  1500|         0|    30|      1500.0|  lowSalary| 1981-09-08|\n",
      "| 7876| ADAMS|    Clerk|7788|12-JAN-1983|  1100|      null|    20|      1100.0|  lowSalary| 1983-01-12|\n",
      "| 7900| JAMES|    Clerk|7698| 3-DEC-1981|   950|      null|    30|       950.0|  lowSalary| 1981-12-03|\n",
      "| 7902|  FORD|  Analyst|7566| 3-DEC-1981|  3000|      null|    20|      3000.0| highSalary| 1981-12-03|\n",
      "| 7934|MILLER|    Clerk|7782|23-JAN-1982|  1300|      null|    10|      1300.0|  lowSalary| 1982-01-23|\n",
      "+-----+------+---------+----+-----------+------+----------+------+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = emp_df.withColumnRenamed('sal','salary').withColumnRenamed('com','commission')\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Writing the dataframe back to disk\n",
    "Below command will write the DF back to disk in the folder `emp_output`. As per Spark and Hadoop functionality, one file is created per worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.coalesce(1).write.csv('emp_output',header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
